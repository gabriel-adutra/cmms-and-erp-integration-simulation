"""
Pipeline principal de integra√ß√£o TracOS ‚Üî Cliente.

Este m√≥dulo orquestra o fluxo completo de sincroniza√ß√£o bidirecional
entre os sistemas Cliente e TracOS, com tratamento robusto de erros,
m√©tricas detalhadas e gest√£o adequada de recursos.
"""

import asyncio
from datetime import datetime
from typing import Dict, Optional
from dataclasses import dataclass
from loguru import logger

from .client_adapter import client_adapter
from .tracos_adapter import tracos_adapter
from .translator import data_translator


@dataclass
class PipelineMetrics:
    """
    M√©tricas de execu√ß√£o do pipeline de integra√ß√£o.
    
    Registra contadores, tempos e taxas de sucesso para monitoramento
    e an√°lise de performance do sistema de integra√ß√£o.
    """
    # Contadores gerais
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Fluxo inbound (Cliente ‚Üí TracOS)
    inbound_files_read: int = 0
    inbound_files_valid: int = 0
    inbound_converted: int = 0
    inbound_saved: int = 0
    inbound_errors: int = 0
    
    # Fluxo outbound (TracOS ‚Üí Cliente)
    outbound_workorders_read: int = 0
    outbound_converted: int = 0
    outbound_saved: int = 0
    outbound_synced: int = 0
    outbound_errors: int = 0
    
    def get_execution_time(self) -> float:
        """Retorna o tempo total de execu√ß√£o em segundos."""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    def get_inbound_success_rate(self) -> float:
        """Retorna a taxa de sucesso do fluxo inbound (0.0 a 1.0)."""
        if self.inbound_files_read == 0:
            return 1.0
        return self.inbound_saved / self.inbound_files_read
    
    def get_outbound_success_rate(self) -> float:
        """Retorna a taxa de sucesso do fluxo outbound (0.0 a 1.0)."""
        if self.outbound_workorders_read == 0:
            return 1.0
        return self.outbound_synced / self.outbound_workorders_read


class IntegrationPipeline:
    """
    Orquestrador principal do pipeline de integra√ß√£o TracOS ‚Üî Cliente.
    
    Esta classe gerencia os fluxos bidirecionais de sincroniza√ß√£o,
    coletando m√©tricas detalhadas, tratando erros de forma isolada
    e garantindo a gest√£o adequada de recursos do sistema.
    """
    
    def __init__(self):
        """Inicializa o pipeline de integra√ß√£o."""
        self.metrics = PipelineMetrics(start_time=datetime.now())
        logger.info("IntegrationPipeline inicializado")
    
    async def _process_single_inbound_file(self, client_data: Dict) -> bool:
        """
        Processa um √∫nico arquivo do fluxo inbound com tratamento de erro isolado.
        
        Args:
            client_data: Dados do cliente a serem processados
            
        Returns:
            True se processado com sucesso, False caso contr√°rio
        """
        try:
            # Valida√ß√£o dos dados do cliente
            if not client_adapter.validate_client_data(client_data):
                logger.warning(f"Dados inv√°lidos para order {client_data.get('orderNo', 'N/A')}")
                return False
            
            self.metrics.inbound_files_valid += 1
            
            # Convers√£o Cliente ‚Üí TracOS
            tracos_data = data_translator.client_to_tracos(client_data)
            self.metrics.inbound_converted += 1
            
            # Salvamento no MongoDB
            success = await tracos_adapter.upsert_workorder(tracos_data)
            if success:
                self.metrics.inbound_saved += 1
                logger.debug(f"Workorder {tracos_data['number']} processada com sucesso")
                return True
            else:
                logger.error(f"Falha ao salvar workorder {tracos_data['number']}")
                return False
                
        except Exception as e:
            self.metrics.inbound_errors += 1
            logger.error(f"Erro ao processar arquivo inbound: {e}")
            return False
    
    async def run_inbound_flow(self) -> Dict:
        """
        Executa o fluxo inbound (Cliente ‚Üí TracOS) com tratamento robusto.
        
        Returns:
            Dicion√°rio com estat√≠sticas do processamento inbound
        """
        logger.info("üîÑ Iniciando fluxo inbound (Cliente ‚Üí TracOS)")
        
        try:
            # Leitura de arquivos do cliente
            files_data = client_adapter.read_inbound_files()
            self.metrics.inbound_files_read = len(files_data)
            
            if not files_data:
                logger.info("Nenhum arquivo encontrado para processamento inbound")
                return self._get_inbound_stats()
            
            logger.info(f"Processando {len(files_data)} arquivo(s) do cliente")
            
            # Processamento individual de cada arquivo
            for i, client_data in enumerate(files_data, 1):
                logger.debug(f"Processando arquivo {i}/{len(files_data)}")
                await self._process_single_inbound_file(client_data)
            
            stats = self._get_inbound_stats()
            logger.info(f"‚úÖ Fluxo inbound conclu√≠do: {stats['success_rate']:.1%} sucesso")
            return stats
            
        except Exception as e:
            logger.error(f"Erro cr√≠tico no fluxo inbound: {e}")
            return self._get_inbound_stats()
    
    async def _process_single_outbound_workorder(self, tracos_data: Dict) -> bool:
        """
        Processa uma √∫nica workorder do fluxo outbound com tratamento de erro isolado.
        
        Args:
            tracos_data: Dados TracOS a serem processados
            
        Returns:
            True se processado com sucesso, False caso contr√°rio
        """
        try:
            workorder_number = tracos_data.get('number', 'N/A')
            
            # Convers√£o TracOS ‚Üí Cliente
            client_data = data_translator.tracos_to_client(tracos_data)
            self.metrics.outbound_converted += 1
            
            # Escrita do arquivo de sa√≠da
            filename = f"workorder_{workorder_number}.json"
            success = client_adapter.write_outbound_file(filename, client_data)
            
            if not success:
                logger.error(f"Falha ao escrever arquivo para workorder {workorder_number}")
                return False
            
            self.metrics.outbound_saved += 1
            
            # Marca√ß√£o como sincronizada
            sync_success = await tracos_adapter.mark_as_synced(workorder_number)
            if sync_success:
                self.metrics.outbound_synced += 1
                logger.debug(f"Workorder {workorder_number} sincronizada com sucesso")
                return True
            else:
                logger.warning(f"Workorder {workorder_number} salva mas n√£o marcada como sincronizada")
                return False
                
        except Exception as e:
            self.metrics.outbound_errors += 1
            logger.error(f"Erro ao processar workorder outbound {tracos_data.get('number', 'N/A')}: {e}")
            return False
    
    async def run_outbound_flow(self) -> Dict:
        """
        Executa o fluxo outbound (TracOS ‚Üí Cliente) com tratamento robusto.
        
        Returns:
            Dicion√°rio com estat√≠sticas do processamento outbound
        """
        logger.info("üîÑ Iniciando fluxo outbound (TracOS ‚Üí Cliente)")
        
        try:
            # Leitura de workorders n√£o sincronizadas
            workorders = await tracos_adapter.read_unsynced_workorders()
            self.metrics.outbound_workorders_read = len(workorders)
            
            if not workorders:
                logger.info("Nenhuma workorder n√£o sincronizada encontrada")
                return self._get_outbound_stats()
            
            logger.info(f"Processando {len(workorders)} workorder(s) n√£o sincronizada(s)")
            
            # Processamento individual de cada workorder
            for i, tracos_data in enumerate(workorders, 1):
                logger.debug(f"Processando workorder {i}/{len(workorders)}")
                await self._process_single_outbound_workorder(tracos_data)
            
            stats = self._get_outbound_stats()
            logger.info(f"‚úÖ Fluxo outbound conclu√≠do: {stats['success_rate']:.1%} sucesso")
            return stats
            
        except Exception as e:
            logger.error(f"Erro cr√≠tico no fluxo outbound: {e}")
            return self._get_outbound_stats()
    
    def _get_inbound_stats(self) -> Dict:
        """Retorna estat√≠sticas do fluxo inbound."""
        return {
            "files_read": self.metrics.inbound_files_read,
            "files_valid": self.metrics.inbound_files_valid,
            "converted": self.metrics.inbound_converted,
            "saved": self.metrics.inbound_saved,
            "errors": self.metrics.inbound_errors,
            "success_rate": self.metrics.get_inbound_success_rate()
        }
    
    def _get_outbound_stats(self) -> Dict:
        """Retorna estat√≠sticas do fluxo outbound."""
        return {
            "workorders_read": self.metrics.outbound_workorders_read,
            "converted": self.metrics.outbound_converted,
            "saved": self.metrics.outbound_saved,
            "synced": self.metrics.outbound_synced,
            "errors": self.metrics.outbound_errors,
            "success_rate": self.metrics.get_outbound_success_rate()
        }
    
    def _log_final_metrics(self, inbound_stats: Dict, outbound_stats: Dict):
        """
        Registra m√©tricas finais estruturadas do pipeline.
        
        Args:
            inbound_stats: Estat√≠sticas do fluxo inbound
            outbound_stats: Estat√≠sticas do fluxo outbound
        """
        execution_time = self.metrics.get_execution_time()
        
        logger.info("üìä === M√âTRICAS FINAIS DO PIPELINE ===")
        logger.info(f"‚è±Ô∏è  Tempo total de execu√ß√£o: {execution_time:.2f} segundos")
        
        # M√©tricas inbound
        logger.info(f"üì• Fluxo Inbound (Cliente ‚Üí TracOS):")
        logger.info(f"   ‚Ä¢ Arquivos lidos: {inbound_stats['files_read']}")
        logger.info(f"   ‚Ä¢ Arquivos v√°lidos: {inbound_stats['files_valid']}")
        logger.info(f"   ‚Ä¢ Convertidos: {inbound_stats['converted']}")
        logger.info(f"   ‚Ä¢ Salvos: {inbound_stats['saved']}")
        logger.info(f"   ‚Ä¢ Erros: {inbound_stats['errors']}")
        logger.info(f"   ‚Ä¢ Taxa de sucesso: {inbound_stats['success_rate']:.1%}")
        
        # M√©tricas outbound
        logger.info(f"üì§ Fluxo Outbound (TracOS ‚Üí Cliente):")
        logger.info(f"   ‚Ä¢ Workorders lidas: {outbound_stats['workorders_read']}")
        logger.info(f"   ‚Ä¢ Convertidas: {outbound_stats['converted']}")
        logger.info(f"   ‚Ä¢ Arquivos salvos: {outbound_stats['saved']}")
        logger.info(f"   ‚Ä¢ Sincronizadas: {outbound_stats['synced']}")
        logger.info(f"   ‚Ä¢ Erros: {outbound_stats['errors']}")
        logger.info(f"   ‚Ä¢ Taxa de sucesso: {outbound_stats['success_rate']:.1%}")
        
        # Status geral
        total_success_rate = (inbound_stats['success_rate'] + outbound_stats['success_rate']) / 2
        logger.info(f"üéØ Taxa de sucesso geral: {total_success_rate:.1%}")
    
    async def execute_full_pipeline(self) -> Dict:
        """
        Executa o pipeline completo de integra√ß√£o com m√©tricas e cleanup.
        
        Returns:
            Dicion√°rio com todas as m√©tricas e resultados da execu√ß√£o
        """
        logger.info("üöÄ === INICIANDO PIPELINE DE INTEGRA√á√ÉO TracOS ‚Üî Cliente ===")
        
        try:
            # Execu√ß√£o dos fluxos
            inbound_stats = await self.run_inbound_flow()
            outbound_stats = await self.run_outbound_flow()
            
            # Finaliza√ß√£o e m√©tricas
            self.metrics.end_time = datetime.now()
            
            # Cleanup de recursos
            await self._cleanup_resources()
            
            # Logging de m√©tricas finais
            self._log_final_metrics(inbound_stats, outbound_stats)
            
            logger.info("üéâ === PIPELINE DE INTEGRA√á√ÉO CONCLU√çDO COM SUCESSO ===")
            
            return {
                "execution_time": self.metrics.get_execution_time(),
                "inbound": inbound_stats,
                "outbound": outbound_stats,
                "overall_success": True
            }
            
        except Exception as e:
            logger.error(f"üí• Falha cr√≠tica no pipeline: {e}")
            await self._cleanup_resources()
            
            return {
                "execution_time": self.metrics.get_execution_time(),
                "error": str(e),
                "overall_success": False
            }
    
    async def _cleanup_resources(self):
        """
        Realiza limpeza de recursos do sistema (conex√µes, caches, etc.).
        
        Garante que recursos como conex√µes MongoDB sejam adequadamente
        fechados ao final da execu√ß√£o do pipeline.
        """
        try:
            # Fechar conex√£o MongoDB se necess√°rio
            await tracos_adapter.close_connection()
            logger.debug("Recursos do sistema limpos com sucesso")
        except Exception as e:
            logger.warning(f"Aviso durante limpeza de recursos: {e}")


# Inst√¢ncia global do pipeline
integration_pipeline = IntegrationPipeline()

# Fun√ß√µes de compatibilidade (mant√™m a interface original)
async def inbound_flow():
    """Fun√ß√£o de compatibilidade - executa fluxo inbound via pipeline global."""
    await integration_pipeline.run_inbound_flow()

async def outbound_flow():
    """Fun√ß√£o de compatibilidade - executa fluxo outbound via pipeline global."""
    await integration_pipeline.run_outbound_flow()

async def main():
    """
    Fun√ß√£o principal que executa o pipeline completo de integra√ß√£o.
    
    Esta √© a fun√ß√£o de entrada do sistema, orquestrando todo o processo
    de sincroniza√ß√£o bidirecional entre Cliente e TracOS.
    """
    await integration_pipeline.execute_full_pipeline()


if __name__ == "__main__":
    asyncio.run(main())
